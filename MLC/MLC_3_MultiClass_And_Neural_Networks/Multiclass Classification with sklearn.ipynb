{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification with sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X] downloading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"[X] downloading data...\")\n",
    "mnist = datasets.fetch_mldata(\"MNIST Original\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size : (52500, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#Temporarily reduce the training set to speed up execution while testing...\n",
    "X_train, X_test, y_train, y_test = train_test_split(mnist.data, mnist.target, test_size = 0.25)\n",
    "print('X_train size :', X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Lambda = 1\n",
    "\n",
    "#lr = LogisticRegression(solver='lbfgs', C=1/Lambda)  # C = 1/lambda (default = 1, if lower, regularization is stronger)\n",
    "\n",
    "test_scores_for_all = np.array([])\n",
    "train_scores_for_all = np.array([])\n",
    "\n",
    "lr_all = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "# One vs all algorithm\n",
    "for value in range(10):\n",
    "    y = np.copy(y_train)\n",
    "    y[y_train==value] = 1\n",
    "    y[y_train!=value] = 0\n",
    "\n",
    "    lr_all[value] = LogisticRegression(solver='lbfgs', C=1/Lambda)  # C = 1/lambda (default = 1, if lower, regularization is stronger)\n",
    "    lr_all[value].fit(X_train,y)\n",
    "    lr_all[value].get_params()\n",
    "\n",
    "    #print('Coeff = {}, intercept = {}'.format(lr.coef_[0], lr.intercept_ ))\n",
    "    print('intercept = {}'.format(lr_all[value].intercept_ ))\n",
    "    #lr_all = np.append(lr_all, np.copy(lr))\n",
    "    print('intercept = {}'.format(lr_all[value].intercept_ ))\n",
    "    print('*** LR ALL ***')\n",
    "    for i in range(value+1):\n",
    "        print('lr_all[{}] = {}'.format(i, lr_all[i].intercept_))\n",
    "    \n",
    "    train_score = lr_all[value].score(X_train, y)\n",
    "    train_scores_for_all = np.append(train_scores_for_all, train_score)\n",
    "    print('Score on training set : ', lr_all[value].score(X_train,y))\n",
    "    \n",
    "    y_test_one = np.copy(y_test)\n",
    "    y_test_one[y_test==value] = 1\n",
    "    y_test_one[y_test!=value] = 0\n",
    " \n",
    "    test_score = lr_all[value].score(X_test,y_test_one)\n",
    "    test_scores_for_all = np.append(test_scores_for_all, test_score)\n",
    "    \n",
    "    print('Score on test set : ', test_score)\n",
    "\n",
    "for value in range(10):\n",
    "    print('lr all[x]', lr_all[value].intercept_)\n",
    "    print('Test score for {} : {}'.format(value, test_scores_for_all[value]))\n",
    "\n",
    "print('Mean performace on training set : ', np.mean(train_scores_for_all))\n",
    "print('Mean performance on test set : ', np.mean(test_scores_for_all))\n",
    "\n",
    "#Note: for 15% datatest, score for training set is 98% and test set : 97.5%\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRandomDigitFrom(n):\n",
    "    r = np.random.randint(0,1000)\n",
    "    img = mnist.data[3000+n*6000+r].reshape(28,28)    \n",
    "    return img\n",
    "\n",
    "guesses = np.array([])\n",
    "confusion = np.empty((0,2))\n",
    "not_found = np.array([])\n",
    "\n",
    "for ntest in range(10000):\n",
    "    digit_to_guess = np.random.randint(0,10)\n",
    "    img = getRandomDigitFrom(digit_to_guess)\n",
    "    #plt.axis('off')\n",
    "    #plt.imshow(img,cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    #plt.show()\n",
    "    img = img.reshape(1,-1)\n",
    "    guessed = 0\n",
    "    wrong_digit = -1\n",
    "    #print('This is a ', digit_to_guess)\n",
    "    for i in range(len(lr_all)):\n",
    "        test = lr_all[i].predict(img) \n",
    "\n",
    "        if test:\n",
    "            #print('I guess this is a ', i)\n",
    "            if ( digit_to_guess == i):\n",
    "                #print('GUESSED !!!')\n",
    "                guessed = 1\n",
    "            else:\n",
    "                wrong_digit = i\n",
    "    \n",
    "    guesses = np.append(guesses, guessed)\n",
    "    if(guessed == 0):\n",
    "        if wrong_digit >= 0:\n",
    "            conf = np.sort(np.array([digit_to_guess, wrong_digit]))\n",
    "            confusion = np.vstack((confusion, conf))\n",
    "        else:\n",
    "            not_found = np.append(not_found,digit_to_guess)\n",
    "        \n",
    "       \n",
    "    \n",
    "w,d = np.unique(guesses, return_inverse=True)\n",
    "#print('confusion: ', confusion)\n",
    "#print('d', d)\n",
    "print('total : ', np.bincount(d))\n",
    "print('perf = {}%'.format(np.bincount(d)[1] / (np.bincount(d)[0] + np.bincount(d)[1]) *100))\n",
    "\n",
    "# Stats on most not found digits:\n",
    "not_found_qty = np.bincount(list(not_found))\n",
    "fig, ax = plt.subplots()\n",
    "plt.bar(range(10), not_found_qty, align='center')\n",
    "plt.xlim(-1,10)\n",
    "ax.set_xticks(range(10))\n",
    "plt.show()\n",
    "\n",
    "not_found_idx = np.ma.argsort(not_found_qty)[::-1]\n",
    "for i in range(not_found_idx.size):\n",
    "    print('{} has been missed {} times'.format(not_found_idx[i], not_found_qty[not_found_idx[i]]))\n",
    "\n",
    "\n",
    "#Stats on confusion:\n",
    "print('Confusion :')\n",
    "plt.scatter(confusion[:,0], confusion[:,1], alpha=0.01)\n",
    "plt.scatter(confusion[:,1], confusion[:,0], alpha=0.01)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Better than using logistic regression, use SVM / SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "classifier = svm.SVC(C=100, gamma=0.001, kernel='poly', probability=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.001, kernel='poly',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total :  [  28 5972]\n",
      "[  28 5972]\n",
      "perf = 99.53333333333333%\n",
      "Confusion :\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEACAYAAACnJV25AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH+FqMkAQY6AKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bf8006b438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getRandomDigitFrom(n):\n",
    "    r = np.random.randint(0,1000)\n",
    "    img = mnist.data[3000+n*6000+r].reshape(28,28)    \n",
    "    return img\n",
    "\n",
    "guesses = np.array([])\n",
    "confusion = np.empty((0,2))\n",
    "not_found = np.array([])\n",
    "\n",
    "for ntest in range(6000):\n",
    "    digit_to_guess = np.random.randint(0,10)\n",
    "    img = getRandomDigitFrom(digit_to_guess)\n",
    "    #plt.axis('off')\n",
    "    #plt.imshow(img,cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    #plt.show()\n",
    "    img = img.reshape(1,-1)\n",
    "    guessed = 0\n",
    "    wrong_digit = -1\n",
    "    #print('This is a ', digit_to_guess)\n",
    "    test = classifier.predict(img)\n",
    "\n",
    "    #print('guessed : ', test)\n",
    "    if test == digit_to_guess:\n",
    "        guessed = 1\n",
    "    else:\n",
    "        wrong_digit = test\n",
    "    \n",
    "    guesses = np.append(guesses, guessed)\n",
    "    if(guessed == 0):\n",
    "        if wrong_digit >= 0:\n",
    "            conf = np.sort(np.array([digit_to_guess, wrong_digit]))\n",
    "            confusion = np.vstack((confusion, conf))\n",
    "        else:\n",
    "            not_found = np.append(not_found,digit_to_guess)\n",
    "        \n",
    "       \n",
    "    \n",
    "w,d = np.unique(guesses, return_inverse=True)\n",
    "#print('confusion: ', confusion)\n",
    "#print('d', d)\n",
    "print('total : ', np.bincount(d))\n",
    "print(np.bincount(d))\n",
    "if np.bincount(d).size == 1:\n",
    "    print('perf = 100%')\n",
    "else:\n",
    "    print('perf = {}%'.format(np.bincount(d)[1] / (np.bincount(d)[0] + np.bincount(d)[1]) *100))\n",
    "\n",
    "# Stats on most not found digits:\n",
    "not_found_qty = np.bincount(list(not_found))\n",
    "#print('not found qty',not_found_qty)\n",
    "\n",
    "if not_found_qty.size > 0:\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(range(10), not_found_qty, align='center')\n",
    "    plt.xlim(-1,10)\n",
    "    ax.set_xticks(range(10))\n",
    "    plt.show()\n",
    "\n",
    "not_found_idx = np.ma.argsort(not_found_qty)[::-1]\n",
    "#print('not found index',not_found_idx)\n",
    "for i in range(not_found_idx.size):\n",
    "    print('{} has been missed {} times'.format(not_found_idx[i], not_found_qty[not_found_idx[i]]))\n",
    "\n",
    "\n",
    "#Stats on confusion:\n",
    "print('Confusion :')\n",
    "plt.scatter(confusion[:,0], confusion[:,1], alpha=0.05)\n",
    "plt.scatter(confusion[:,1], confusion[:,0], alpha=0.05)\n",
    "plt.show()\n",
    "\n",
    "# on 15% training \n",
    "# C=1000, gamma = 0.001 => 92% perf\n",
    "# C=100, gamma = 0.001 => 92% perf \n",
    "# C=10, gamma = 0.001 => 91.6%\n",
    "# C=1, gamma = 0.001 => 92.2%\n",
    "# C=0.01, gamma = 0.001 => 92%\n",
    "\n",
    "# C=1, gamma = 1 => 92% perf\n",
    "# C=1, gamma = 1000 => 91%\n",
    "# C=1, gamma = 0.000001 => 92%\n",
    "\n",
    "# SO... no real difference... \n",
    "# Sigmoid => 24% !??\n",
    "# Poly => 93% !!!\n",
    "\n",
    "#Poly !! (on 15% data)\n",
    "# C=1, gamma = 0.000001 => 96% !!\n",
    "# C=1, gamma = 0.001 => 96.5% !!\n",
    "# C=100, gamma = 0.001 => 96.8% !!\n",
    "# on 30% Data :\n",
    "# C=100, gamma = 0.001 => 98.5% !! => When classifier.score is used, we get 97%, because in my case, I also use indifferently trained data, that biaised the results\n",
    "# C=100, gamma = 0.001 + 75% data => 99.5% !!! score 97.8% \n",
    "# C=100, gamma = 0.001 => Score 97.8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97822857142857145"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAABZRJREFUeJzt3b9LlX0cxvHnPBo1iFBDQX+DiIOB4dQSNDVEodkk6OiU\nTTYkru01trbWalBDhejmPyBkOTYUDv44z9DyDPI51Ok+t3q9XqMXnfseevMdvh3rdLvdf4As/7b9\nAsDgCR8CCR8CCR8CCR8CCR8CCR8CDQ/gGf6hALSnc9IPnfgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQ\nSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQ\nSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQSPgQaLjtF+B8e/nyZbl/+fJlQG/yZx4/flzu\nly5dKvehoaFyHx5uJ0EnPgQSPgQSPgQSPgQSPgQSPgQSPgTqdLvdpp/R+ANoz/b2drnfvn273L99\n+/Y3X+fUuXnzZrl//Pix6VfonPRDJz4EEj4EEj4EEj4EEj4EEj4EEj4E8n18SgcHB+W+sLBQ7uf9\nnr6Xa9eutf0KJ3LiQyDhQyDhQyDhQyDhQyDhQyDhQyD3+JR6fV/88+fPA3qTk129erXce/1e+8nJ\nyXK/c+fOb7/T/y0uLvb155vixIdAwodAwodAwodAwodAwodAwodA7vEpvX37ttHPv3XrVrnPzc2V\n+/3798t9dHT0t98pgRMfAgkfAgkfAgkfAgkfAgkfAgkfAnW63cb/+/rGH8Cf29jYKPe7d++W+97e\nXrlPTEyU+/r6erlfuXKl3Ompc9IPnfgQSPgQSPgQSPgQSPgQSPgQSPgQyPfxz7n9/f1yX15eLvde\n9/S9LC0tlbt7+nY48SGQ8CGQ8CGQ8CGQ8CGQ8CGQ8CGQe/xz7tmzZ+X+4cOHvj6/1++tv3HjRl+f\nTzOc+BBI+BBI+BBI+BBI+BBI+BBI+BDIPf4Zt7OzU+6vXr1q9Pn37t0r97GxsUafz59x4kMg4UMg\n4UMg4UMg4UMg4UMg4UMg9/hn3IsXL8q939+LPz4+Xu7Pnz/v6/NphxMfAgkfAgkfAgkfAgkfAgkf\nAgkfArnHP+V+/vxZ7p8+fWr0+ZcvX+5r53Ry4kMg4UMg4UMg4UMg4UMg4UMg4UMg9/in3NraWrm/\nf/++0efPz883+vm0w4kPgYQPgYQPgYQPgYQPgYQPgYQPgdzjn3K7u7utPv/69eutPp9mOPEhkPAh\nkPAhkPAhkPAhkPAhkPAhkHv8ln39+rXc37x50+jzZ2dny31qaqrR59MOJz4EEj4EEj4EEj4EEj4E\nEj4EEj4Eco/fsq2trXL//v17o89/9OhRuY+MjDT6fNrhxIdAwodAwodAwodAwodAwodAwodA8ff4\nDx8+LPfNzc1yf/LkSbn/+PGj3J8+fVru0AQnPgQSPgQSPgQSPgQSPgQSPgQSPgTqdLvdpp/R+AP6\n0el02n6FRo2Pj5f7xsZGuV+8ePFvvg6Dd+JfcCc+BBI+BBI+BBI+BBI+BBI+BBI+BIr/Pv7r16/L\n/cGDBwN6k2ZMT0+Xu3v6TE58CCR8CCR8CCR8CCR8CCR8CCR8CBT/ffzDw8NyX1lZKff19fVy7/V7\n+WdmZsq91z386upqub97967cx8bGyp0zz/fxgV+ED4GED4GED4GED4GED4GED4Hi7/H7dXR0VO7H\nx8flPjQ0VO69fu9/r3+HcOHChXLn3HOPD/wifAgkfAgkfAgkfAgkfAgkfAjkHh/ON/f4wC/Ch0DC\nh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DCh0DC\nh0DCh0DDA3hG/R+8AwPnxIdAwodAwodAwodAwodAwodAwodAwodAwodAwodAwodAwodAwodAwodA\nwodAwodAwodAwodAwodAwodA/wFP/qX5CggLXQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2bf80c35518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proba img :  [[ 0.00238421  0.68634654  0.00515044  0.00150117  0.00100029  0.16871632\n",
      "   0.03777855  0.00335596  0.09255981  0.00120672]]\n"
     ]
    }
   ],
   "source": [
    "img = getRandomDigitFrom(5)\n",
    "plt.axis('off')\n",
    "plt.imshow(img,cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()\n",
    "img = img.reshape(1,-1)\n",
    "\n",
    "proba_img = classifier.predict_proba(img)\n",
    "print('proba img : ', proba_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
