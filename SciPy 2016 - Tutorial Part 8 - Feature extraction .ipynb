{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>FEATURE EXTRACTION is an important topic of MACHINE LEARNING</H2>\n",
    "\n",
    "How features are extracted from real world data\n",
    "\n",
    "Example of extracting numerical features from textual data\n",
    "\n",
    "In scikit learn, data are expected to be n_samples x n_features\n",
    "\n",
    "Nominal features != categorial features. \n",
    "\n",
    "Categorial features can be ordered, compared, when ordinal can't really. For IRIS dataset, it means that petal length is categorial, while the color is Nominal. The algo assumes that features are categorial, so there is a workaround to avoid the algo to treat them as categorial : one-hot encoding representation : in this case, we select specific color (purple, blue, red) that have meaning with value of 1.0 if the color is exactly this one, and 0 if it is the opposite (NB : leads to sparse matrix)\n",
    "\n",
    "DictVectorizer encode categorical features\n",
    "\n",
    "\n",
    "<h3>DictVectorizer</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measurements = [\n",
    "    {'city': 'Duba√Ø', 'temperature': 33.},\n",
    "    {'city': 'London', 'temperature': 12.},\n",
    "    {'city': 'San Francisco', 'temperature': 18.}\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec = DictVectorizer()\n",
    "print('vectorizer parameters :', vec)\n",
    "\n",
    "print(vec.fit_transform(measurements).toarray())\n",
    "\n",
    "print('labels:', vec.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Derive features</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "titanic = pd.read_csv(os.path.join('datasets', 'titanic3.csv'))\n",
    "print(titanic.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont want to use boat and body in a classification survived/not survived because they already contain this information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As domain expert, we must know and understand wether values like integers are nominal or categorical. In the case the class is typical categorical and needs to be treated as this. In this case, the class, number 1, 2 or 3, will be translated into three different columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = titanic.survived.values\n",
    "features =  titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
    "\n",
    "features.head()\n",
    "\n",
    "# We need to transform 'sex' and 'embarked' into binary data\n",
    "pd.get_dummies(features).head()\n",
    "\n",
    "# pclass is already binary however it is also categorial, so we need to specify it to the encoder\n",
    "features_dummies = pd.get_dummies(features, columns=['pclass', 'sec', 'embarked'])\n",
    "features_dummies.head(n=16)\n",
    "\n",
    "data = features_dummies.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have missing values, we can simply get rid of the column, or, if we want to use it, we can also simply assign the mean value to the unknown ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-bcd2583afb5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isnan(data).any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, randam_state = 0)\n",
    "\n",
    "imp = Imputer()\n",
    "imp.fit(train_data)\n",
    "train_data_finite = imp.transform(train_data)\n",
    "test_data_finite = imp.transform(test_data)\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier('most_frequent')\n",
    "clf.fit(train_data_finite, train_labels)\n",
    "print('Prediction accuracy: %f', clf.score(test_data_finite, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use some better classifier like Logistic Regression and Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(train_data_finite, train_labels)\n",
    "print('logistic regression score : ', lr.score(test_data_finite, test_labels))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state = 0).fit(train_data_finite, train_labels)\n",
    "print('RandomForest classifier score : ', rf.score(test_data_finite, test_labels))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
