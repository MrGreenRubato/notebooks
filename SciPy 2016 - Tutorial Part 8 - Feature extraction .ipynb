{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>FEATURE EXTRACTION is an important topic of MACHINE LEARNING</H2>\n",
    "\n",
    "How features are extracted from real world data\n",
    "\n",
    "Example of extracting numerical features from textual data\n",
    "\n",
    "In scikit learn, data are expected to be n_samples x n_features\n",
    "\n",
    "Nominal features != categorial features. \n",
    "\n",
    "Categorial features can be ordered, compared, when ordinal can't really. For IRIS dataset, it means that petal length is categorial, while the color is Nominal. The algo assumes that features are categorial, so there is a workaround to avoid the algo to treat them as categorial : one-hot encoding representation : in this case, we select specific color (purple, blue, red) that have meaning with value of 1.0 if the color is exactly this one, and 0 if it is the opposite (NB : leads to sparse matrix)\n",
    "\n",
    "DictVectorizer encode categorical features\n",
    "\n",
    "\n",
    "<h3>DictVectorizer</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer parameters : DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
      "        sparse=True)\n",
      "[[  1.   0.   0.  33.]\n",
      " [  0.   1.   0.  12.]\n",
      " [  0.   0.   1.  18.]]\n",
      "labels: ['city=Dubaï', 'city=London', 'city=San Francisco', 'temperature']\n"
     ]
    }
   ],
   "source": [
    "measurements = [\n",
    "    {'city': 'Dubaï', 'temperature': 33.},\n",
    "    {'city': 'London', 'temperature': 12.},\n",
    "    {'city': 'San Francisco', 'temperature': 18.}\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec = DictVectorizer()\n",
    "print('vectorizer parameters :', vec)\n",
    "\n",
    "print(vec.fit_transform(measurements).toarray())\n",
    "\n",
    "print('labels:', vec.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Derive features</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket',\n",
      "       'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#titanic = pd.read_csv(os.path.join('datasets', 'titanic3.csv'))\n",
    "titanic = pd.read_csv('files/titanic3.csv')\n",
    "print(titanic.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont want to use boat and body in a classification survived/not survived because they already contain this information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As domain expert, we must know and understand wether values like integers are nominal or categorical. In the case the class is typical categorical and needs to be treated as this. In this case, the class, number 1, 2 or 3, will be translated into three different columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      age  sibsp  parch      fare  pclass_1  pclass_2  pclass_3  sex_female  \\\n",
      "0   29.00      0      0  211.3375       1.0       0.0       0.0         1.0   \n",
      "1    0.92      1      2  151.5500       1.0       0.0       0.0         0.0   \n",
      "2    2.00      1      2  151.5500       1.0       0.0       0.0         1.0   \n",
      "3   30.00      1      2  151.5500       1.0       0.0       0.0         0.0   \n",
      "4   25.00      1      2  151.5500       1.0       0.0       0.0         1.0   \n",
      "5   48.00      0      0   26.5500       1.0       0.0       0.0         0.0   \n",
      "6   63.00      1      0   77.9583       1.0       0.0       0.0         1.0   \n",
      "7   39.00      0      0    0.0000       1.0       0.0       0.0         0.0   \n",
      "8   53.00      2      0   51.4792       1.0       0.0       0.0         1.0   \n",
      "9   71.00      0      0   49.5042       1.0       0.0       0.0         0.0   \n",
      "10  47.00      1      0  227.5250       1.0       0.0       0.0         0.0   \n",
      "11  18.00      1      0  227.5250       1.0       0.0       0.0         1.0   \n",
      "12  24.00      0      0   69.3000       1.0       0.0       0.0         1.0   \n",
      "13  26.00      0      0   78.8500       1.0       0.0       0.0         1.0   \n",
      "14  80.00      0      0   30.0000       1.0       0.0       0.0         0.0   \n",
      "15    NaN      0      0   25.9250       1.0       0.0       0.0         0.0   \n",
      "\n",
      "    sex_male  embarked_C  embarked_Q  embarked_S  \n",
      "0        0.0         0.0         0.0         1.0  \n",
      "1        1.0         0.0         0.0         1.0  \n",
      "2        0.0         0.0         0.0         1.0  \n",
      "3        1.0         0.0         0.0         1.0  \n",
      "4        0.0         0.0         0.0         1.0  \n",
      "5        1.0         0.0         0.0         1.0  \n",
      "6        0.0         0.0         0.0         1.0  \n",
      "7        1.0         0.0         0.0         1.0  \n",
      "8        0.0         0.0         0.0         1.0  \n",
      "9        1.0         1.0         0.0         0.0  \n",
      "10       1.0         1.0         0.0         0.0  \n",
      "11       0.0         1.0         0.0         0.0  \n",
      "12       0.0         1.0         0.0         0.0  \n",
      "13       0.0         0.0         0.0         1.0  \n",
      "14       1.0         0.0         0.0         1.0  \n",
      "15       1.0         0.0         0.0         1.0  \n",
      "[[ 29.     0.     0.   ...,   0.     0.     1.  ]\n",
      " [  0.92   1.     2.   ...,   0.     0.     1.  ]\n",
      " [  2.     1.     2.   ...,   0.     0.     1.  ]\n",
      " ..., \n",
      " [ 26.5    0.     0.   ...,   1.     0.     0.  ]\n",
      " [ 27.     0.     0.   ...,   1.     0.     0.  ]\n",
      " [ 29.     0.     0.   ...,   0.     0.     1.  ]]\n",
      "data shape  (1309, 12)\n"
     ]
    }
   ],
   "source": [
    "labels = titanic.survived.values\n",
    "features =  titanic[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
    "\n",
    "features.head()\n",
    "\n",
    "# We need to transform 'sex' and 'embarked' into binary data\n",
    "pd.get_dummies(features).head()\n",
    "\n",
    "# pclass is already binary however it is also categorial, so we need to specify it to the encoder\n",
    "features_dummies = pd.get_dummies(features, columns=['pclass', 'sex', 'embarked'])\n",
    "print(features_dummies.head(n=16))\n",
    "\n",
    "data = features_dummies.values\n",
    "\n",
    "print(data)\n",
    "print('data shape ', data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have missing values, we can simply get rid of the column, or, if we want to use it, we can also simply assign the mean value to the unknown ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.isnan(data).any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy:  0.634146341463\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, random_state = 0)\n",
    "\n",
    "imp = Imputer()\n",
    "imp.fit(train_data)\n",
    "train_data_finite = imp.transform(train_data)\n",
    "test_data_finite = imp.transform(test_data)\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "clf = DummyClassifier('most_frequent')\n",
    "clf.fit(train_data_finite, train_labels)\n",
    "print('Prediction accuracy: ', clf.score(test_data_finite, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use some better classifier like Logistic Regression and Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression score :  0.792682926829\n",
      "RandomForest classifier score :  0.77743902439\n",
      "logistic regression score w/o embark&parch:  0.789634146341\n",
      "RandomForest classifier score w/o embark&parch:  0.80487804878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(train_data_finite, train_labels)\n",
    "print('logistic regression score : ', lr.score(test_data_finite, test_labels))\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state = 0).fit(train_data_finite, train_labels)\n",
    "print('RandomForest classifier score : ', rf.score(test_data_finite, test_labels))\n",
    "\n",
    "\n",
    "features_dummies_sub = pd.get_dummies(features[['pclass', 'sex', 'age', 'sibsp', 'fare']])\n",
    "data_sub = features_dummies_sub.values\n",
    "\n",
    "train_data_sub, test_data_sub, train_labels, test_labels = train_test_split(data_sub, labels, random_state = 0)\n",
    "\n",
    "imp = Imputer()\n",
    "imp.fit(train_data_sub)\n",
    "train_data_finite_sub = imp.transform(train_data_sub)\n",
    "test_data_finite_sub = imp.transform(test_data_sub)\n",
    "\n",
    "lr = LogisticRegression().fit(train_data_finite_sub, train_labels)\n",
    "print('logistic regression score w/o embark&parch: ', lr.score(test_data_finite_sub, test_labels))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, random_state = 0).fit(train_data_finite_sub, train_labels)\n",
    "print('RandomForest classifier score w/o embark&parch: ', rf.score(test_data_finite_sub, test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
